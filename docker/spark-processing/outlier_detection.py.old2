from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, udf, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType
from pyspark.ml.linalg import Vectors
import joblib
import numpy as np
import logging

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("KafkaOutlierDetection") \
    .config("spark.cassandra.connection.host", "cassandra") \
    .config("spark.cassandra.connection.port", "9042") \
    .config("spark.cassandra.auth.username", "cassandra") \
    .config("spark.cassandra.auth.password", "cassandra") \
    .config("spark.jars.packages", "com.datastax.spark:spark-cassandra-connector_2.12:3.1.0") \
    .getOrCreate()

# Initialize logger
logger = logging.getLogger("KafkaOutlierDetection")
logging.basicConfig(level=logging.INFO)

# Load the Isolation Forest model and scaler
model_path = "/isolation_forest_model.joblib"
scaler_path = "/scaler.joblib"
try:
    model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)
    logger.info("Model and scaler loaded successfully.")
except Exception as e:
    logger.error(f"Error loading model or scaler: {e}")
    raise

# Broadcast the model and scaler to the Spark cluster
broadcast_model = spark.sparkContext.broadcast(model)
broadcast_scaler = spark.sparkContext.broadcast(scaler)

# Define the schema matching the structure of the JSON messages
schema = StructType([
    StructField("station_id", StringType()),
    StructField("humidity", DoubleType()),
    StructField("noise_level", DoubleType()),
    StructField("temperature", DoubleType()),
])

# Read data from Kafka
kafka_df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "broker:9092") \
    .option("subscribe", "iot-sensor-stream") \
    .load()

# Parse the JSON data from Kafka
parsed_df = kafka_df.selectExpr("CAST(value AS STRING)").select(from_json(col("value"), schema).alias("data")).select("data.*")

# Convert features to vector
def to_vector(humidity, noise_level, temperature):
    return Vectors.dense([humidity, noise_level, temperature])

vector_udf = udf(to_vector)

# Apply the vectorization
vector_df = parsed_df.withColumn("features", vector_udf(col("humidity"), col("noise_level"), col("temperature")))

# UDF to predict outliers using Isolation Forest
def predict_outliers(features):
    model = broadcast_model.value
    scaler = broadcast_scaler.value
    try:
        features_np = np.array(features).reshape(1, -1).tolist()
        features_scaled = scaler.transform(features_np)
        predictions = model.predict(features_scaled)
        return predictions[0] == -1
    except Exception as e:
        logger.error(f"Error in outlier prediction: {e}")
        return False

predict_outliers_udf = udf(predict_outliers, BooleanType())

# Apply the outlier detection model
outliers_df = vector_df.withColumn("is_outlier", predict_outliers_udf(col("features")))

# Select only the columns that are present in the Cassandra table schema
outliers_df = outliers_df.select("station_id", "humidity", "noise_level", "temperature", "is_outlier")

# Query to print results to the console
console_query = outliers_df.writeStream \
    .outputMode("update") \
    .format("console") \
    .start()

# Define the query to write the streaming data to Cassandra
# def write_to_cassandra(batch_df, epoch_id):
#     try:
#         # Add a timestamp column to the DataFrame
#         batch_with_timestamp = batch_df.withColumn("record_time", current_timestamp())
#         batch_with_timestamp.write \
#             .format("org.apache.spark.sql.cassandra") \
#             .mode("append") \
#             .option("keyspace", "iot_stations") \
#             .option("table", "spark_stream") \
#             .save()
#     except Exception as e:
#         logger.error(f"Error writing to Cassandra: {e}")

# cassandra_query = outliers_df.writeStream \
#     .outputMode("update") \
#     .foreachBatch(write_to_cassandra) \
#     .start()

# Wait for both streaming queries to terminate
console_query.awaitTermination()
# cassandra_query.awaitTermination()
